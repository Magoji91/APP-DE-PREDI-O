setwd("C:/Users/User/Documents/en_US")

require(tm)
require(SnowballC)
require(RWeka)
require(data.table)
require(stringr)
library(tidytext)
library(dplyr)
library(data.table)



##READ THE FILES AND GET MORE INFORMATION:
news <- readLines(file("en_US.news.txt"))
blogs <- readLines(file("en_US.blogs.txt"))
twitter<- readLines(file("en_US.twitter.txt"))

# PREPARE THE DATA FRAME
data.frame(source = c("blogs", "news", "twitter"),         
           file.size.MB = c(blogs.size, news.size, twitter.size),
           num.lines = c(length(blogs), length(news), length(twitter)),
           num.words = c(sum(blogs.words), sum(news.words), sum(twitter.words)),
           mean.num.words = c(mean(blogs.words), mean(news.words), mean(twitter.words)))

# REMOVE LARGE FILE TO CLEAN UP THE MEMORY
To prevent memory from being clogged by intermediate files use the rm command
```{r save}
rm (blogs.words)
rm(news.words)
rm(twitter.words)

rm(blogs)
rm(news)
rm(twitter)
          
####Functions to remove special characters
detectNonAsciiChar <- function(x) iconv(x, from="UTF-8", to="ASCII",sub="X")##detects a nonASCII character and substitutesit with "X"
removeNonAsciiWord <- function(x) gsub("[a-z]*X+[a-z]*", " ", x)##removes the detected nonASCII characters
removeHTTPS <- function(x) gsub("https://(.*)[.][a-z]+|https://[a-z]+", " ", x)
removeHTTP  <- function(x) gsub("http://(.*)[.][a-z]+|https://[a-z]+", " ", x)
removeFTP <- function(x) gsub("ftp://(.*)[.][a-z]+|https://[a-z]+", " ", x)
removeWWW <- function(x) gsub("www(.*)[.][a-z]+|www.", " ", x)
removeHashTag <- function(x) gsub("#[a-z0-9]+", " ", x)
removeTwitterRT <- function(x) gsub("^rt |^rt:", " ", x)
removeCharRepetition <- function(x) {
    a <- gsub("[a-z]*aaa[a-z]*", " ", x)
    a <- gsub("[a-z]*bbb[a-z]*", " ", a)
    a <- gsub("[a-z]*ccc[a-z]*", " ", a)
    a <- gsub("[a-z]*ddd[a-z]*", " ", a)
    a <- gsub("[a-z]*eee[a-z]*", " ", a)
    a <- gsub("[a-z]*fff[a-z]*", " ", a)
    a <- gsub("[a-z]*ggg[a-z]*", " ", a)
    a <- gsub("[a-z]*hhh[a-z]*", " ", a)
    a <- gsub("[a-z]*iii[a-z]*", " ", a)
    a <- gsub("[a-z]*jjj[a-z]*", " ", a)
    a <- gsub("[a-z]*kkk[a-z]*", " ", a)
    a <- gsub("[a-z]*lll[a-z]*", " ", a)
    a <- gsub("[a-z]*mmm[a-z]*", " ", a)
    a <- gsub("[a-z]*nnn[a-z]*", " ", a)
    a <- gsub("[a-z]*ooo[a-z]*", " ", a)
    a <- gsub("[a-z]*ppp[a-z]*", " ", a)
    a <- gsub("[a-z]*qqq[a-z]*", " ", a)
    a <- gsub("[a-z]*rrr[a-z]*", " ", a)
    a <- gsub("[a-z]*sss[a-z]*", " ", a)
    a <- gsub("[a-z]*ttt[a-z]*", " ", a)
    a <- gsub("[a-z]*uuu[a-z]*", " ", a)
    a <- gsub("[a-z]*vvv[a-z]*", " ", a)
    a <- gsub("[a-z]*www[a-z]*", " ", a)
    a <- gsub("[a-z]*xxx[a-z]*", " ", a)
    a <- gsub("[a-z]*yyy[a-z]*", " ", a)
    gsub("[a-z]*zzz[a-z]*", " ", a)
}

##Clean the combined file
library(tm)
merged <- paste(news[1:5000], blogs[1:5000], twitter[1:5000])
corpus <- VCorpus(VectorSource(merged))
corpus<- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(detectNonAsciiChar))
corpus <- tm_map(corpus, content_transformer(removeNonAsciiWord))
corpus<- tm_map(corpus, content_transformer(removeHTTPS))
corpus <- tm_map(corpus, content_transformer(removeHTTP))
corpus <- tm_map(corpus, content_transformer(removeFTP))
corpus <- tm_map(corpus, content_transformer(removeWWW))
corpus <- tm_map(corpus, content_transformer(removeHashTag))
corpus <- tm_map(corpus, content_transformer(removeTwitterRT))
corpus<- tm_map(corpus, content_transformer(removeCharRepetition))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus , stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

##Load list of profanity
badWordsURL<-"https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
download.file(badWordsURL,destfile="badWords.txt")
badWords<-read.csv("badWords.txt",header=FALSE,sep="\n",strip.white=TRUE)
corpus<-tm_map(corpus,removeWords,badWords[,1])       

library(RWeka)
corpusDf <-data.frame(text=unlist(sapply(corpus, 
                                         `[`, "content")), stringsAsFactors=F)

####Preparing a tidy text document
tidy_docs<-tidy(allSamp)
###Generating frquencies
############uni grams##################################################
token_uni<-tidy_docs %>% unnest_tokens(words,text)

freq_uni<-token_uni %>% count(words, sort=TRUE)
uiL<-str_split(freq_uni$words," ")
freq_uni$start<-sapply(uiL,FUN=function(x) x[1])
freq_uni$end<-sapply(uiL,FUN=function(x) x[1])
rm(token_uni, uiL)


######################bi grams#####################################################################################
token_bi<-tidy_docs %>% unnest_tokens(bigram,text,token="ngrams", n=2)

freq_bi<-token_bi %>% count(bigram, sort=TRUE)
biL<-str_split(freq_bi$bigram," ")
freq_bi$start<-sapply(biL,FUN=function(x) x[1])
freq_bi$end<-sapply(biL,FUN=function(x) x[2])
rm(token_bi, biL)

######################tri grams#####################################################################################
token_tri<-tidy_docs %>% unnest_tokens(trigram,text,token="ngrams", n=3)

freq_tri<-token_tri %>% count(trigram, sort=TRUE)
triL<-str_split(freq_tri$trigram," ")
freq_tri$start<-sapply(triL,FUN=function(x) paste(x[1],x[2]))
freq_tri$end<-sapply(triL,FUN=function(x) x[3])

rm(token_tri, triL)

######################quadra grams#####################################################################################
token_quadi<-tidy_docs %>% unnest_tokens(quadragram,text,token="ngrams", n=4)

freq_quadi<-token_quadi %>% count(quadragram, sort=TRUE)
quadiL<-str_split(freq_quadi$quadragram," ")
freq_quadi$start<-sapply(quadiL,FUN=function(x) paste(x[1],x[2],x[3]))
freq_quadi$end<-sapply(quadiL,FUN=function(x) x[4])
rm(token_quadi, quadiL)

######################penta grams#####################################################################################
token_penta<-tidy_docs %>% unnest_tokens(pentagram,text,token="ngrams", n=5)

freq_penta<-token_penta %>% count(pentagram, sort=TRUE)
pentaL<-str_split(freq_penta$pentagram," ")
freq_penta$start<-sapply(pentaL,FUN=function(x) paste(x[1],x[2],x[3],x[4]))
freq_penta$end<-sapply(pentaL,FUN=function(x) x[5])
rm(token_penta, pentaL)


rm(corpus)

########################################################################################################################
##Clean up lower frequency terms to reduce size of datasets####################################################################################
freq_bi<-subset(freq_bi, freq_bi$n>=2)
freq_tri<-subset(freq_tri, freq_tri$n>=2)
freq_quadi<-subset(freq_quadi, freq_quadi$n>=2)
freq_penta<-subset(freq_penta, freq_penta$n>=2)

########################Write the frequency datasets to file; to be used later for app upload#########################

write.csv(freq_uni, file="unigrams.csv", row.names = F)
write.csv(freq_bi, file="bigrams.csv", row.names = F)
write.csv(freq_tri, file="trigrams.csv", row.names = F)
write.csv(freq_quadi, file="quadragrams.csv", row.names = F)
write.csv(freq_penta, file="pentagrams.csv", row.names = F)
